# TransAttUnet: Lung Nodule Segmentation on LIDC-IDRI

Triá»ƒn khai mÃ´ hÃ¬nh TransAttUnet (Multi-level Attention-guided U-Net with Transformer) Ä‘á»ƒ phÃ¢n vÃ¹ng ná»‘t phá»•i trÃªn táº­p dá»¯ liá»‡u CT phá»•i LIDC-IDRI.

Dá»± Ã¡n Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch, bao gá»“m quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u 3D sang 2D, module Self-aware Attention (SAA) vÃ  chiáº¿n lÆ°á»£c huáº¥n luyá»‡n tá»‘i Æ°u cho dá»¯ liá»‡u y táº¿.

1. Kiáº¿n trÃºc vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a mÃ´ hÃ¬nh

MÃ´ hÃ¬nh TransAttUnet kháº¯c phá»¥c háº¡n cháº¿ cá»§a U-Net truyá»n thá»‘ng chá»‰ nhÃ¬n cá»¥c bá»™ báº±ng cÃ¡ch káº¿t há»£p cÆ¡ cháº¿ Attention toÃ n cá»¥c. Kiáº¿n trÃºc gá»“m 3 pháº§n chÃ­nh:

A. Encoder (Feature Extraction)

- Sá»­ dá»¥ng kiáº¿n trÃºc CNN nhiá»u táº§ng Ä‘á»ƒ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« áº£nh CT $512 \times 512$.

- Giáº£m kÃ­ch thÆ°á»›c khÃ´ng gian vÃ  tÄƒng chiá»u sÃ¢u (channels) Ä‘á»ƒ náº¯m báº¯t ngá»¯ nghÄ©a cáº¥p cao.

B. Self-aware Attention (SAA) Bridge

Cáº§u ná»‘i giá»¯a Encoder vÃ  Decoder. NÃ³ gá»“m 2 nhÃ¡nh song song:

Transformer Self Attention (TSA): GiÃºp mÃ´ hÃ¬nh nhÃ¬n toÃ n bá»™ bá»©c áº£nh cÃ¹ng lÃºc Ä‘á»ƒ náº¯m báº¯t má»‘i quan há»‡ táº§m xa, vÃ­ dá»¥ liÃªn káº¿t giá»¯a cÃ¡c vÃ¹ng phá»•i khÃ¡c nhau.

Global Spatial Attention (GSA): MÃ£ hÃ³a thÃ´ng tin vá»‹ trÃ­ khÃ´ng gian, giÃºp mÃ´ hÃ¬nh Ä‘á»‹nh vá»‹ chÃ­nh xÃ¡c vÃ¹ng ná»‘t phá»•i.

C. Decoder & Multi-scale Skip Connections

- KhÃ´i phá»¥c kÃ­ch thÆ°á»›c áº£nh vá» $512 \times 512$ Ä‘á»ƒ táº¡o máº·t náº¡ (mask).

- Sá»­ dá»¥ng Residual Skip Connections: Thay vÃ¬ ná»‘i ghÃ©p Ä‘Æ¡n giáº£n, mÃ´ hÃ¬nh cá»™ng gá»™p thÃ´ng tin tá»« cÃ¡c táº§ng Encoder Ä‘á»ƒ giá»¯ láº¡i chi tiáº¿t biÃªn cáº¡nh sáº¯c nÃ©t cá»§a khá»‘i u.

2. CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

Äáº£m báº£o Ä‘Ã£ cÃ i Ä‘áº·t Python 3.8+.
```bash
    git clone https://github.com/yourusername/TransAttUnet-LIDC.git
    cd TransAttUnet-LIDC
    python -m venv venv# Windows:
    venv\Scripts\activate
    
    pip install -r requirements.txt
```
3. Chuáº©n bá»‹ dá»¯ liá»‡u & Preprocessing

Dá»¯ liá»‡u Ä‘áº§u vÃ o lÃ  táº­p LIDC-IDRI (Ä‘á»‹nh dáº¡ng DICOM). Quy trÃ¬nh Preprocessing Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u 3D phá»©c táº¡p thÃ nh áº£nh 2D sáº¡ch cho mÃ´ hÃ¬nh.

CÃ¡c bÆ°á»›c Preprocessing chi tiáº¿t:

- Resampling: Äá»“ng nháº¥t Ä‘á»™ phÃ¢n giáº£i khÃ´ng gian vá» $1mm \times 1mm \times 1mm$ (Isotropic) Ä‘á»ƒ trÃ¡nh mÃ©o hÃ¬nh.

- Intensity Windowing: Ãp dá»¥ng Lung Window Ä‘á»ƒ lÃ m rÃµ nhu mÃ´ phá»•i:

Window Center: -600 HU

Window Width: 1500 HU

- Chuáº©n hÃ³a giÃ¡ trá»‹ pixel vá» khoáº£ng [0, 1].

- Consensus Masking: Táº¡o nhÃ£n (Ground Truth) tá»« 4 bÃ¡c sÄ©. Sá»­ dá»¥ng má»©c Ä‘á»“ng thuáº­n 50% (Ã­t nháº¥t 2 bÃ¡c sÄ© Ä‘á»“ng Ã½ lÃ  ná»‘t phá»•i).

- Slicing vÃ  Filtering: Cáº¯t khá»‘i 3D thÃ nh cÃ¡c lÃ¡t cáº¯t 2D ($512 \times 512$). Chá»‰ giá»¯ láº¡i cÃ¡c lÃ¡t cáº¯t cÃ³ chá»©a ná»‘t phá»•i (Positive Slices).

CÃ¡ch cháº¡y Preprocessing:
```bash
    # Xá»­ lÃ½ toÃ n bá»™ dá»¯ liá»‡u
    
    python preprocess.py
    
    python preprocess.py --num 10
    
    tar -czf processed.tar.gz data/processed
```

Náº¿u train trÃªn modal thÃ¬ cháº¡y cell giáº£i nÃ©n Ä‘á»ƒ train cho dá»…

```bash
    import os
    
    volume_root = "/mnt/model"
    target_dir = os.path.join(volume_root, "data", "processed")
    
    # 1. Táº¡o thÆ° má»¥c Ä‘Ã­ch náº¿u chÆ°a cÃ³
    os.makedirs(target_dir, exist_ok=True)
    print(f"ÄÃ£ sáºµn sÃ ng thÆ° má»¥c: {target_dir}")
    
    # 2. ÄÆ°á»ng dáº«n file tar
    tar_path = os.path.join(volume_root, "processed.tar.gz")
    
    if not os.path.exists(tar_path):
        print("KhÃ´ng tÃ¬m tháº¥y file processed.tar.gz â€“ báº¡n Ä‘Ã£ upload chÆ°a?")
    else:
        print("Äang giáº£i nÃ©n processed.tar.gz vá»›i strip 2 cáº¥p ... (chá» 20â€“60 giÃ¢y)")
        
        # Lá»‡nh chÃ­nh: strip-components=2 Ä‘á»ƒ bá» cáº¥p data/processed/
        !tar -xzf "{tar_path}" -C "{target_dir}" --strip-components=2
        
        print("GIáº¢I NÃ‰N XONG 100%! Ná»™i dung Ä‘Ã£ náº±m Ä‘Ãºng trong data/processed/")
    
    # 3. Kiá»ƒm tra káº¿t quáº£
    print("\nHOÃ€N Táº¤T! CÃ¡c item trong data/processed:")
    items = os.listdir(target_dir)
    print(f"Tá»•ng cá»™ng: {len(items)} items")
    print("30 item Ä‘áº§u tiÃªn:")
    for item in sorted(items)[:30]:
        print(f"  - {item}")
    
    # 4. (TÃ¹y chá»n) XÃ³a file tar Ä‘á»ƒ tiáº¿t kiá»‡m dung lÆ°á»£ng Volume
    # !rm "{tar_path}"
    # print(f"ÄÃ£ xÃ³a file tar gá»‘c")
    
    print("\nSáº´N SÃ€NG TRAIN MODEL! ðŸš€")
```

Káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u vÃ o thÆ° má»¥c data/processed/ gá»“m:

images/: File .npy chá»©a áº£nh CT.

masks/: File .npy chá»©a nhÃ£n phÃ¢n vÃ¹ng.

split.json: File chia táº­p Train/Val/Test.

4. Kiá»ƒm tra dá»¯ liá»‡u

TrÆ°á»›c khi train, hÃ£y kiá»ƒm tra xem áº£nh vÃ  mask cÃ³ khá»›p nhau khÃ´ng:

```bash
    python check_data.py
```
-> Hiá»ƒn thá»‹ ngáº«u nhiÃªn bá»™ 3 hÃ¬nh: áº¢nh gá»‘c - Mask - áº¢nh chá»“ng (Overlay) Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng.

5. Training

MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i hÃ m loss káº¿t há»£p: Loss = 0.5 * BCE + 0.5 * Dice.

    Cáº¥u hÃ¬nh (configs/config.yaml)
    
    CÃ³ thá»ƒ thay Ä‘á»•i cÃ¡c tham sá»‘ quan trá»ng:
    
    batch_size: Máº·c Ä‘á»‹nh 4 (TÄƒng lÃªn 8/16 náº¿u VRAM > 12GB).
    
    lr (Learning Rate): Máº·c Ä‘á»‹nh 0.001 (cho SGD).
    
    epochs: Máº·c Ä‘á»‹nh 100.
```bash
    python train.py
```
Tiáº¿p tá»¥c huáº¥n luyá»‡n (Resume):

Náº¿u quÃ¡ trÃ¬nh train bá»‹ ngáº¯t quÃ£ng, cÃ³ thá»ƒ cháº¡y tiáº¿p tá»« checkpoint gáº§n nháº¥t:
```bash
    python train.py --resume outputs/checkpoints/last_checkpoint.pth
```